{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization\n",
    "\n",
    "When working with text provided by nltk, documents are already tokenized. When working with our own text, we often need to create tokens.\n",
    "\n",
    "Tokenization - the act of breaking a text document into individual words or sentences\n",
    "\n",
    "Sentence tokens - Tokens divided on punctuation\n",
    "\n",
    "Word tokens - Tokens divided on spaces or puncuation. (Every item is a word or punctuation symbol)\n",
    "\n",
    "Working with tokens makes analyzing text much easier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens:  ['I', 'am', 'learning', 'Natural', 'Language', 'Processing', '.']\n"
     ]
    }
   ],
   "source": [
    "my_string = \"I am learning Natural Language Processing.\"\n",
    "\n",
    "tokens = nltk.word_tokenize(my_string)\n",
    "print(\"Tokens: \", tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens: 7\n"
     ]
    }
   ],
   "source": [
    "#Original string is now a list of tokens\n",
    "num_tokens = len(tokens)\n",
    "print(\"Number of tokens:\", num_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence tokens:  ['I am learning Natural Language Processing.', 'I am learning how to tokenize!']\n"
     ]
    }
   ],
   "source": [
    "phrase = \"I am learning Natural Language Processing. I am learning how to tokenize!\"\n",
    "tokens_sent = nltk.sent_tokenize(phrase)\n",
    "\n",
    "print(\"Sentence tokens: \", tokens_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sentence:  I am learning Natural Language Processing.\n",
      "Sentence tokenized:  ['I', 'am', 'learning', 'Natural', 'Language', 'Processing', '.']\n",
      "\n",
      "Sentence:  I am learning how to tokenize!\n",
      "Sentence tokenized:  ['I', 'am', 'learning', 'how', 'to', 'tokenize', '!']\n"
     ]
    }
   ],
   "source": [
    "#Tokenize sentence tokens\n",
    "\n",
    "for item in tokens_sent:\n",
    "    print(\"\\nSentence: \", item)\n",
    "    tokens_from_sent_tokens = nltk.word_tokenize(item)\n",
    "    print(\"Sentence tokenized: \", tokens_from_sent_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalizing\n",
    "- the act of cleaning our text data to make it more uniform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 22 words:  ['[', 'Moby', 'Dick', 'by', 'Herman', 'Melville', '1851', ']', 'ETYMOLOGY', '.', '(', 'Supplied', 'by', 'a', 'Late', 'Consumptive', 'Usher', 'to', 'a', 'Grammar', 'School', ')']\n"
     ]
    }
   ],
   "source": [
    "md = nltk.corpus.gutenberg.words(\"melville-moby_dick.txt\")\n",
    "\n",
    "md_22 = md[:22]\n",
    "print(\"First 22 words: \", md_22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moby\n",
      "Dick\n",
      "by\n",
      "Herman\n",
      "Melville\n",
      "ETYMOLOGY\n",
      "Supplied\n",
      "by\n",
      "a\n",
      "Late\n",
      "Consumptive\n",
      "Usher\n",
      "to\n",
      "a\n",
      "Grammar\n",
      "School\n"
     ]
    }
   ],
   "source": [
    "for word in md_22:\n",
    "    if word.isalpha(): #Is alpha is a boolean function that returns True if the string is alphabetic and False if it is not.\n",
    "        print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 22 words in lower case:  ['[', 'moby', 'dick', 'by', 'herman', 'melville', '1851', ']', 'etymology', '.', '(', 'supplied', 'by', 'a', 'late', 'consumptive', 'usher', 'to', 'a', 'grammar', 'school', ')']\n"
     ]
    }
   ],
   "source": [
    "#Make all our text the same case\n",
    "md_22_lower = [word.lower() for word in md_22]\n",
    "print(\"First 22 words in lower case: \", md_22_lower)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 22 words in lower case and only alpha:  ['moby', 'dick', 'by', 'herman', 'melville', 'etymology', 'supplied', 'by', 'a', 'late', 'consumptive', 'usher', 'to', 'a', 'grammar', 'school']\n"
     ]
    }
   ],
   "source": [
    "md_22_norm = [word.lower() for word in md_22 if word.isalpha()]\n",
    "print(\"First 22 words in lower case and only alpha: \", md_22_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several ways to normalize even further\n",
    "\n",
    "Suppose we want to remove affixes from our words, we can accomplish this with something called stemers.\n",
    "Some of these help when we want to know if cats is referenced as well as cat "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat\n",
      "cat\n",
      "lie\n",
      "lie\n",
      "run\n",
      "run\n",
      "citi\n",
      "citi\n",
      "month\n",
      "monthli\n",
      "woman\n",
      "women\n"
     ]
    }
   ],
   "source": [
    "porter = nltk.PorterStemmer()\n",
    "\n",
    "plurals_list = [\"cat\", \"cats\", \"lie\", \"lying\", \"run\", \"running\", \"city\", \"cities\", \"month\", \"monthly\", \"woman\", \"women\"]\n",
    "\n",
    "for word in plurals_list:\n",
    "    print(porter.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat\n",
      "cat\n",
      "lie\n",
      "lying\n",
      "run\n",
      "run\n",
      "city\n",
      "city\n",
      "mon\n",
      "month\n",
      "wom\n",
      "wom\n"
     ]
    }
   ],
   "source": [
    "lancaster = nltk.LancasterStemmer()\n",
    "\n",
    "for word in plurals_list:\n",
    "    print(lancaster.stem(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One more method to solve normalization problem called lemmatization.\n",
    "\n",
    "We can use the word net resource provided by nltk to remove the affixes.\n",
    "**Note**: More computationally expensive "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat\n",
      "cat\n",
      "lie\n",
      "lying\n",
      "run\n",
      "running\n",
      "city\n",
      "city\n",
      "month\n",
      "monthly\n",
      "woman\n",
      "woman\n"
     ]
    }
   ],
   "source": [
    "wordnet_lem = nltk.WordNetLemmatizer()\n",
    "\n",
    "for word in plurals_list:\n",
    "    print (wordnet_lem.lemmatize(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part of Speech Tagging\n",
    "\n",
    "May discover that knowing the part of speech is required to solve a given NLP problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tagged tokens:  [('I', 'PRP'), ('walked', 'VBD'), ('to', 'TO'), ('the', 'DT'), ('cafe', 'NN'), ('to', 'TO'), ('buy', 'VB'), ('coffee', 'NN'), ('before', 'IN'), ('work', 'NN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "tag_phrase = \"I walked to the cafe to buy coffee before work.\"\n",
    "\n",
    "tag_tokens = nltk.word_tokenize(tag_phrase)\n",
    "\n",
    "tagged = nltk.pos_tag(tag_tokens)\n",
    "\n",
    "print(\"Tagged tokens: \", tagged)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have an assigned part of speech to every token in our phrase.\n",
    "\n",
    "Run code block below for descriptions on what each tag means."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$: dollar\n",
      "    $ -$ --$ A$ C$ HK$ M$ NZ$ S$ U.S.$ US$\n",
      "'': closing quotation mark\n",
      "    ' ''\n",
      "(: opening parenthesis\n",
      "    ( [ {\n",
      "): closing parenthesis\n",
      "    ) ] }\n",
      ",: comma\n",
      "    ,\n",
      "--: dash\n",
      "    --\n",
      ".: sentence terminator\n",
      "    . ! ?\n",
      ":: colon or ellipsis\n",
      "    : ; ...\n",
      "CC: conjunction, coordinating\n",
      "    & 'n and both but either et for less minus neither nor or plus so\n",
      "    therefore times v. versus vs. whether yet\n",
      "CD: numeral, cardinal\n",
      "    mid-1890 nine-thirty forty-two one-tenth ten million 0.5 one forty-\n",
      "    seven 1987 twenty '79 zero two 78-degrees eighty-four IX '60s .025\n",
      "    fifteen 271,124 dozen quintillion DM2,000 ...\n",
      "DT: determiner\n",
      "    all an another any both del each either every half la many much nary\n",
      "    neither no some such that the them these this those\n",
      "EX: existential there\n",
      "    there\n",
      "FW: foreign word\n",
      "    gemeinschaft hund ich jeux habeas Haementeria Herr K'ang-si vous\n",
      "    lutihaw alai je jour objets salutaris fille quibusdam pas trop Monte\n",
      "    terram fiche oui corporis ...\n",
      "IN: preposition or conjunction, subordinating\n",
      "    astride among uppon whether out inside pro despite on by throughout\n",
      "    below within for towards near behind atop around if like until below\n",
      "    next into if beside ...\n",
      "JJ: adjective or numeral, ordinal\n",
      "    third ill-mannered pre-war regrettable oiled calamitous first separable\n",
      "    ectoplasmic battery-powered participatory fourth still-to-be-named\n",
      "    multilingual multi-disciplinary ...\n",
      "JJR: adjective, comparative\n",
      "    bleaker braver breezier briefer brighter brisker broader bumper busier\n",
      "    calmer cheaper choosier cleaner clearer closer colder commoner costlier\n",
      "    cozier creamier crunchier cuter ...\n",
      "JJS: adjective, superlative\n",
      "    calmest cheapest choicest classiest cleanest clearest closest commonest\n",
      "    corniest costliest crassest creepiest crudest cutest darkest deadliest\n",
      "    dearest deepest densest dinkiest ...\n",
      "LS: list item marker\n",
      "    A A. B B. C C. D E F First G H I J K One SP-44001 SP-44002 SP-44005\n",
      "    SP-44007 Second Third Three Two * a b c d first five four one six three\n",
      "    two\n",
      "MD: modal auxiliary\n",
      "    can cannot could couldn't dare may might must need ought shall should\n",
      "    shouldn't will would\n",
      "NN: noun, common, singular or mass\n",
      "    common-carrier cabbage knuckle-duster Casino afghan shed thermostat\n",
      "    investment slide humour falloff slick wind hyena override subhumanity\n",
      "    machinist ...\n",
      "NNP: noun, proper, singular\n",
      "    Motown Venneboerger Czestochwa Ranzer Conchita Trumplane Christos\n",
      "    Oceanside Escobar Kreisler Sawyer Cougar Yvette Ervin ODI Darryl CTCA\n",
      "    Shannon A.K.C. Meltex Liverpool ...\n",
      "NNPS: noun, proper, plural\n",
      "    Americans Americas Amharas Amityvilles Amusements Anarcho-Syndicalists\n",
      "    Andalusians Andes Andruses Angels Animals Anthony Antilles Antiques\n",
      "    Apache Apaches Apocrypha ...\n",
      "NNS: noun, common, plural\n",
      "    undergraduates scotches bric-a-brac products bodyguards facets coasts\n",
      "    divestitures storehouses designs clubs fragrances averages\n",
      "    subjectivists apprehensions muses factory-jobs ...\n",
      "PDT: pre-determiner\n",
      "    all both half many quite such sure this\n",
      "POS: genitive marker\n",
      "    ' 's\n",
      "PRP: pronoun, personal\n",
      "    hers herself him himself hisself it itself me myself one oneself ours\n",
      "    ourselves ownself self she thee theirs them themselves they thou thy us\n",
      "PRP$: pronoun, possessive\n",
      "    her his mine my our ours their thy your\n",
      "RB: adverb\n",
      "    occasionally unabatingly maddeningly adventurously professedly\n",
      "    stirringly prominently technologically magisterially predominately\n",
      "    swiftly fiscally pitilessly ...\n",
      "RBR: adverb, comparative\n",
      "    further gloomier grander graver greater grimmer harder harsher\n",
      "    healthier heavier higher however larger later leaner lengthier less-\n",
      "    perfectly lesser lonelier longer louder lower more ...\n",
      "RBS: adverb, superlative\n",
      "    best biggest bluntest earliest farthest first furthest hardest\n",
      "    heartiest highest largest least less most nearest second tightest worst\n",
      "RP: particle\n",
      "    aboard about across along apart around aside at away back before behind\n",
      "    by crop down ever fast for forth from go high i.e. in into just later\n",
      "    low more off on open out over per pie raising start teeth that through\n",
      "    under unto up up-pp upon whole with you\n",
      "SYM: symbol\n",
      "    % & ' '' ''. ) ). * + ,. < = > @ A[fj] U.S U.S.S.R * ** ***\n",
      "TO: \"to\" as preposition or infinitive marker\n",
      "    to\n",
      "UH: interjection\n",
      "    Goodbye Goody Gosh Wow Jeepers Jee-sus Hubba Hey Kee-reist Oops amen\n",
      "    huh howdy uh dammit whammo shucks heck anyways whodunnit honey golly\n",
      "    man baby diddle hush sonuvabitch ...\n",
      "VB: verb, base form\n",
      "    ask assemble assess assign assume atone attention avoid bake balkanize\n",
      "    bank begin behold believe bend benefit bevel beware bless boil bomb\n",
      "    boost brace break bring broil brush build ...\n",
      "VBD: verb, past tense\n",
      "    dipped pleaded swiped regummed soaked tidied convened halted registered\n",
      "    cushioned exacted snubbed strode aimed adopted belied figgered\n",
      "    speculated wore appreciated contemplated ...\n",
      "VBG: verb, present participle or gerund\n",
      "    telegraphing stirring focusing angering judging stalling lactating\n",
      "    hankerin' alleging veering capping approaching traveling besieging\n",
      "    encrypting interrupting erasing wincing ...\n",
      "VBN: verb, past participle\n",
      "    multihulled dilapidated aerosolized chaired languished panelized used\n",
      "    experimented flourished imitated reunifed factored condensed sheared\n",
      "    unsettled primed dubbed desired ...\n",
      "VBP: verb, present tense, not 3rd person singular\n",
      "    predominate wrap resort sue twist spill cure lengthen brush terminate\n",
      "    appear tend stray glisten obtain comprise detest tease attract\n",
      "    emphasize mold postpone sever return wag ...\n",
      "VBZ: verb, present tense, 3rd person singular\n",
      "    bases reconstructs marks mixes displeases seals carps weaves snatches\n",
      "    slumps stretches authorizes smolders pictures emerges stockpiles\n",
      "    seduces fizzes uses bolsters slaps speaks pleads ...\n",
      "WDT: WH-determiner\n",
      "    that what whatever which whichever\n",
      "WP: WH-pronoun\n",
      "    that what whatever whatsoever which who whom whosoever\n",
      "WP$: WH-pronoun, possessive\n",
      "    whose\n",
      "WRB: Wh-adverb\n",
      "    how however whence whenever where whereby whereever wherein whereof why\n",
      "``: opening quotation mark\n",
      "    ` ``\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(nltk.help.upenn_tagset())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tagged example phrase:  [('I', 'PRP'), ('will', 'MD'), ('have', 'VB'), ('dessert', 'NN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "example_phrase = \"I will have dessert.\"\n",
    "tagged_example_phrase = nltk.pos_tag(nltk.word_tokenize(example_phrase))\n",
    "\n",
    "print(\"Tagged example phrase: \", tagged_example_phrase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words tagged:  218361\n",
      "Tagged words:  [('moby', 'NOUN'), ('dick', 'NOUN'), ('by', 'ADP'), ('herman', 'NOUN'), ('melville', 'NOUN'), ('etymology', 'NOUN'), ('supplied', 'VERB'), ('by', 'ADP'), ('a', 'DET'), ('late', 'ADJ'), ('consumptive', 'NOUN'), ('usher', 'NOUN'), ('to', 'PRT'), ('a', 'DET'), ('grammar', 'NOUN'), ('school', 'NOUN'), ('the', 'DET'), ('pale', 'NOUN'), ('usher', 'NOUN'), ('threadbare', 'NOUN')]\n"
     ]
    }
   ],
   "source": [
    "#Find most common nouns in moby_dick\n",
    "md = nltk.corpus.gutenberg.words(\"melville-moby_dick.txt\")\n",
    "md_norm = [word.lower() for word in md if word.isalpha()]\n",
    "\n",
    "#We are not concerned with types of nouns so we will tell tagger to be less descriptive.\n",
    "\n",
    "md_tags = nltk.pos_tag(md_norm, tagset=\"universal\")\n",
    "\n",
    "print(\"Number of words tagged: \", len(md_norm))\n",
    "print(\"Tagged words: \", md_tags[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MD nouns:  ['moby', 'dick', 'herman', 'melville', 'etymology', 'consumptive', 'usher', 'grammar', 'school', 'pale', 'usher', 'threadbare', 'heart', 'body', 'brain', 'i', 'lexicons', 'grammars', 'queer', 'handkerchief']\n"
     ]
    }
   ],
   "source": [
    "md_nouns = [word for word, tag in md_tags if tag == \"NOUN\"]\n",
    "\n",
    "print(\"MD nouns: \", md_nouns[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most common nouns:  [('i', 1182), ('whale', 909), ('s', 774), ('man', 527), ('ship', 498), ('sea', 435), ('head', 337), ('time', 334), ('boat', 332), ('ahab', 278), ('captain', 275), ('way', 271), ('whales', 256), ('men', 244), ('ye', 220), ('hand', 214), ('side', 197), ('water', 190), ('deck', 189), ('thing', 188)]\n"
     ]
    }
   ],
   "source": [
    "#Frequency distribution to find most common nouns\n",
    "md_nouns_fd = nltk.FreqDist(md_nouns)\n",
    "\n",
    "print(\"Most common nouns: \", md_nouns_fd.most_common(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example: Multiple Parts of Speech\n",
    "\n",
    "In this example, we will be quantifying words that have different parts of speech based on usage.\n",
    "\n",
    "For the Alice in Wonderland text, find all parts of speech used for the words 'over', 'spoke', and 'answer'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alice word tags conditional frequency distribution:  dict_items([('over', FreqDist({'ADP': 31, 'PRT': 5, 'ADV': 4})), ('spoke', FreqDist({'VERB': 16, 'NOUN': 1})), ('answer', FreqDist({'NOUN': 5, 'VERB': 3, 'ADP': 1}))])\n"
     ]
    }
   ],
   "source": [
    "alice = nltk.corpus.gutenberg.words(\"carroll-alice.txt\")\n",
    "\n",
    "alice_norm = [word.lower() for word in alice if word.isalpha()]\n",
    "\n",
    "alice_tags = nltk.pos_tag(alice_norm, tagset=\"universal\")\n",
    "\n",
    "alice_word_tags = [(word, tag) for word, tag in alice_tags if word in [\"over\", \"spoke\", \"answer\"]]\n",
    "\n",
    "alice_tags_cd = nltk.ConditionalFreqDist(alice_word_tags)\n",
    "\n",
    "print(\"Alice word tags conditional frequency distribution: \", alice_tags_cd.items())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example: Choices\n",
    "Lets say we wanted to find all cases in a given text where there was a choice between two options.\n",
    "This would be of the form \\<noun\\> or \\<noun\\>.\n",
    "\n",
    "Task: Load up the bryant-stories.txt and find every time there is a choice presented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tags [('[', 'NOUN'), ('Stories', 'NOUN'), ('to', 'PRT'), ('Tell', 'VERB'), ('to', 'PRT'), ('Children', 'NOUN'), ('by', 'ADP'), ('Sara', 'NOUN'), ('Cone', 'NOUN'), ('Bryant', 'NOUN')]\n"
     ]
    }
   ],
   "source": [
    "bryant_stories = nltk.corpus.gutenberg.words(\"bryant-stories.txt\")\n",
    "\n",
    "tags = nltk.pos_tag(bryant_stories, tagset=\"universal\")\n",
    "\n",
    "print(\"tags\", tags[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ship or part\n",
      "food or water\n",
      "queens or princesses\n",
      "rank or wealth\n"
     ]
    }
   ],
   "source": [
    "choices = []\n",
    "for ((word1, tag1), (word2, tag2), (word3, tag3)) in nltk.trigrams(tags):\n",
    "    if tag1 == \"NOUN\" and word2 == \"or\" and tag3 == \"NOUN\":\n",
    "        choices.append((word1, word3))\n",
    "        print(word1 + \" \" + word2 + \" \" + word3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chunking\n",
    "\n",
    "How to solve when words get split up in ways we don't want them to. e.g. 'New' 'York'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunking_tags [('I', 'PRP'), ('will', 'MD'), ('go', 'VB'), ('to', 'TO'), ('the', 'DT'), ('coffee', 'NN'), ('shop', 'NN'), ('in', 'IN'), ('New', 'NNP'), ('York', 'NNP')]\n"
     ]
    }
   ],
   "source": [
    "chunking_phrase = \"I will go to the coffee shop in New York after I get off the jet plane.\"\n",
    "\n",
    "chunking_tags = nltk.pos_tag(nltk.word_tokenize(chunking_phrase))\n",
    "\n",
    "print(\"chunking_tags\", chunking_tags[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence = '''\n",
    "    Chunk:\n",
    "    {<NNPS>+}\n",
    "    {<NNP>+}\n",
    "    {<NN>+}'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  I/PRP\n",
      "  will/MD\n",
      "  go/VB\n",
      "  to/TO\n",
      "  the/DT\n",
      "  (Chunk coffee/NN shop/NN)\n",
      "  in/IN\n",
      "  (Chunk New/NNP York/NNP)\n",
      "  after/IN\n",
      "  I/PRP\n",
      "  get/VBP\n",
      "  off/IN\n",
      "  the/DT\n",
      "  (Chunk jet/NN plane/NN)\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "NPChunker = nltk.RegexpParser(sequence)\n",
    "result = NPChunker.parse(chunking_tags)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Named Entity Recognition\n",
    "\n",
    "How to use chunking to find named entities (People, locations, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text:  World War II (WWII or WW2), also known as the Second World War, was a global war that lasted from 1939 to 1945, though related conflicts began earlier. It involved the vast majority of the world's nationsâ€”including all of the great powersâ€”eventually forming two opposing military alliances: the Allies and the Axis. It was the most widespread war in history, and directly involved more than 100 million people from over 30 countries. In a state of \"total war\", the major participants threw their entire economic, industrial, and scientific capabilities behind the war effort, erasing the distinction between civilian and military resources. Marked by mass deaths of civilians, including the Holocaust (in which approximately 11 million people were killed) and the strategic bombing of industrial and population centres (in which approximately one million were killed, and which included the atomic bombings of Hiroshima and Nagasaki), it resulted in an estimated 50 million to 85 million fatalities. These made World War II the deadliest conflict in human history.\n",
      "\n",
      "The Empire of Japan aimed to dominate Asia and the Pacific and was already at war with the Republic of China in 1937, but the world war is generally said to have begun on 1 September 1939 with the invasion of Poland by Germany and subsequent declarations of war on Germany by France and the United Kingdom. From late 1939 to early 1941, in a series of campaigns and treaties, Germany conquered or controlled much of continental Europe, and formed the Axis alliance with Italy and Japan. Based on the Molotovâ€“Ribbentrop Pact of August 1939, Germany and the Soviet Union partitioned and annexed territories of their European neighbours, Poland, Finland, Romania and the Baltic states. For a year starting in late June 1940, the United Kingdom and the British Commonwealth were the only Allied forces continuing the fight against the European Axis powers, with campaigns in North Africa and the Horn of Africa, the aerial Battle of Britain and the Blitz bombing campaign, as well as the long-running Battle of the Atlantic. In June 1941, the European Axis powers launched an invasion of the Soviet Union, opening the largest land theatre of war in history, which trapped the major part of the Axis' military forces into a war of attrition. In December 1941, Japan attacked the United States and European territories in the Pacific Ocean, and quickly conquered much of the Western Pacific.\n",
      "\n",
      "The Axis advance halted in 1942 when Japan lost the critical Battle of Midway, near Hawaii, and Germany was defeated in North Africa and then, decisively, at Stalingrad in the Soviet Union. In 1943, with a series of German defeats on the Eastern Front, the Allied invasion of Italy which brought about Italian surrender, and Allied victories in the Pacific, the Axis lost the initiative and undertook strategic retreat on all fronts. In 1944, the Western Allies invaded German-occupied France, while the Soviet Union regained all of its territorial losses and invaded Germany and its allies. During 1944 and 1945 the Japanese suffered major reverses in mainland Asia in South Central China and Burma, while the Allies crippled the Japanese Navy and captured key Western Pacific islands.\n",
      "\n",
      "The war in Europe ended with an invasion of Germany by the Western Allies and the Soviet Union culminating in the capture of Berlin by Soviet and Polish troops and the subsequent German unconditional surrender on 8 May 1945. Following the Potsdam Declaration by the Allies on 26 July 1945 and the refusal of Japan to surrender under its terms, the United States dropped atomic bombs on the Japanese cities of Hiroshima and Nagasaki on 6 August and 9 August respectively. With an invasion of the Japanese archipelago imminent, the possibility of additional atomic bombings, and the Soviet Union's declaration of war on Japan and invasion of Manchuria, Japan surrendered on 15 August 1945. Thus ended the war in Asia, cementing the total victory of the Allies.\n",
      "\n",
      "World War II altered the political alignment and social structure of the world. The United Nations (UN) was established to foster international co-operation and prevent future conflicts. The victorious great powersâ€”the United States, the Soviet Union, China, the United Kingdom, and Franceâ€”became the permanent members of the United Nations Security Council. The Soviet Union and the United States emerged as rival superpowers, setting the stage for the Cold War, which lasted for the next 46 years. Meanwhile, the influence of European great powers waned, while the decolonisation of Asia and Africa began. Most countries whose industries had been damaged moved towards economic recovery. Political integration, especially in Europe, emerged as an effort to end pre-war enmities and to create a common identity.\n"
     ]
    }
   ],
   "source": [
    "text = open(\"example.txt\").read()\n",
    "print(\"Text: \", text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORGANIZATION WWII\n",
      "ORGANIZATION WW2\n",
      "ORGANIZATION Second\n",
      "ORGANIZATION Axis\n",
      "ORGANIZATION Hiroshima\n",
      "GPE Nagasaki\n",
      "ORGANIZATION Empire of Japan\n",
      "GPE Asia\n",
      "ORGANIZATION Pacific\n",
      "ORGANIZATION Republic\n",
      "GPE China\n",
      "GPE Poland\n",
      "GPE Germany\n",
      "GPE Germany\n",
      "GPE France\n",
      "ORGANIZATION United Kingdom\n",
      "GPE Germany\n",
      "GPE Europe\n",
      "ORGANIZATION Axis\n",
      "GPE Italy\n",
      "GPE Japan\n",
      "GPE Germany\n",
      "GPE Soviet Union\n",
      "GPE European\n",
      "GPE Poland\n",
      "GPE Finland\n",
      "GPE Romania\n",
      "GPE Baltic\n",
      "ORGANIZATION United Kingdom\n",
      "GPE British\n",
      "ORGANIZATION European Axis\n",
      "GPE North Africa\n",
      "ORGANIZATION Horn\n",
      "GPE Africa\n",
      "GPE Britain\n",
      "GPE Blitz\n",
      "ORGANIZATION Atlantic\n",
      "ORGANIZATION European Axis\n",
      "GPE Soviet Union\n",
      "ORGANIZATION Axis\n",
      "GPE Japan\n",
      "GPE United States\n",
      "GPE European\n",
      "ORGANIZATION Pacific Ocean\n",
      "LOCATION Western Pacific\n",
      "ORGANIZATION Axis\n",
      "PERSON Japan\n",
      "GPE Midway\n",
      "GPE Hawaii\n",
      "GPE Germany\n",
      "GPE North Africa\n",
      "FACILITY Stalingrad\n",
      "GPE Soviet Union\n",
      "GPE German\n",
      "LOCATION Eastern Front\n",
      "GPE Italy\n",
      "GPE Italian\n",
      "GPE Allied\n",
      "ORGANIZATION Pacific\n",
      "ORGANIZATION Axis\n",
      "LOCATION Western\n",
      "GPE France\n",
      "GPE Soviet Union\n",
      "GPE Germany\n",
      "GPE Japanese\n",
      "GPE Asia\n",
      "GPE South\n",
      "GPE China\n",
      "GPE Burma\n",
      "GPE Japanese\n",
      "ORGANIZATION Navy\n",
      "LOCATION Western Pacific\n",
      "GPE Europe\n",
      "GPE Germany\n",
      "LOCATION Western\n",
      "GPE Soviet Union\n",
      "GPE Berlin\n",
      "GPE Soviet\n",
      "GPE Polish\n",
      "GPE German\n",
      "ORGANIZATION Potsdam\n",
      "GPE Japan\n",
      "GPE United States\n",
      "GPE Japanese\n",
      "ORGANIZATION Hiroshima\n",
      "PERSON Nagasaki\n",
      "GPE Japanese\n",
      "GPE Soviet Union\n",
      "GPE Japan\n",
      "GPE Manchuria\n",
      "GPE Japan\n",
      "GPE Asia\n",
      "ORGANIZATION United Nations\n",
      "GPE United States\n",
      "GPE Soviet Union\n",
      "GPE China\n",
      "ORGANIZATION United Kingdom\n",
      "ORGANIZATION United Nations\n",
      "ORGANIZATION Security Council\n",
      "GPE Soviet Union\n",
      "GPE United States\n",
      "GPE European\n",
      "GPE Asia\n",
      "PERSON Africa\n",
      "GPE Europe\n"
     ]
    }
   ],
   "source": [
    "text_tag = nltk.pos_tag(nltk.word_tokenize(text))\n",
    "\n",
    "text_chunks = nltk.ne_chunk(text_tag)\n",
    "\n",
    "for chunk in text_chunks:\n",
    "    if hasattr(chunk, 'label'):\n",
    "        print(chunk.label(), ' '.join(c[0] for c in chunk.leaves()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
